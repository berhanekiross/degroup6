{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jupyter lab --ip=0.0.0.0\n",
    "# start-master.sh\n",
    "# start-worker.sh spark://yash-kukrejade-6:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/13 10:00:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "# from F import col, explode, lower, split\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://waldo-ruiz-master-de-6:7077\") \\\n",
    "        .appName(\"Lecture1_Example0_with_spark\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/13 10:06:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 221.5 KiB, free 433.2 MiB)\n",
      "24/03/13 10:06:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.1 MiB)\n",
      "24/03/13 10:06:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on waldo-ruiz-master-de-6:10005 (size: 32.6 KiB, free: 434.2 MiB)\n",
      "24/03/13 10:06:07 INFO SparkContext: Created broadcast 8 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/13 10:06:07 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/13 10:06:07 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/13 10:06:07 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/13 10:06:07 INFO DAGScheduler: Final stage: ResultStage 4 (runJob at PythonRDD.scala:181)\n",
      "24/03/13 10:06:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/13 10:06:07 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/13 10:06:07 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[14] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/13 10:06:07 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.7 KiB, free 433.1 MiB)\n",
      "24/03/13 10:06:07 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 433.1 MiB)\n",
      "24/03/13 10:06:07 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on waldo-ruiz-master-de-6:10005 (size: 4.8 KiB, free: 434.2 MiB)\n",
      "24/03/13 10:06:07 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/13 10:06:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[14] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/13 10:06:07 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/03/13 10:06:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (192.168.2.113, executor 2, partition 0, PROCESS_LOCAL, 7671 bytes) \n",
      "24/03/13 10:06:07 INFO BlockManagerInfo: Removed broadcast_5_piece0 on waldo-ruiz-master-de-6:10005 in memory (size: 4.8 KiB, free: 434.2 MiB)\n",
      "24/03/13 10:06:07 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.113:10006 in memory (size: 4.8 KiB, free: 434.4 MiB)\n",
      "24/03/13 10:06:07 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.113:10006 (size: 4.8 KiB, free: 434.4 MiB)\n",
      "24/03/13 10:06:07 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 16) (192.168.2.113 executor 2): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n",
      "\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/13 10:06:07 INFO TaskSetManager: Starting task 0.1 in stage 4.0 (TID 17) (192.168.2.113, executor 2, partition 0, PROCESS_LOCAL, 7671 bytes) \n",
      "24/03/13 10:06:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on waldo-ruiz-master-de-6:10005 in memory (size: 4.8 KiB, free: 434.2 MiB)\n",
      "24/03/13 10:06:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.113:10006 in memory (size: 4.8 KiB, free: 434.4 MiB)\n",
      "24/03/13 10:06:07 INFO TaskSetManager: Lost task 0.1 in stage 4.0 (TID 17) on 192.168.2.113, executor 2: java.io.InvalidClassException (org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982) [duplicate 1]\n",
      "24/03/13 10:06:07 INFO TaskSetManager: Starting task 0.2 in stage 4.0 (TID 18) (192.168.2.113, executor 2, partition 0, PROCESS_LOCAL, 7671 bytes) \n",
      "24/03/13 10:06:07 INFO TaskSetManager: Lost task 0.2 in stage 4.0 (TID 18) on 192.168.2.113, executor 2: java.io.InvalidClassException (org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982) [duplicate 2]\n",
      "24/03/13 10:06:07 INFO TaskSetManager: Starting task 0.3 in stage 4.0 (TID 19) (192.168.2.113, executor 2, partition 0, PROCESS_LOCAL, 7671 bytes) \n",
      "24/03/13 10:06:07 INFO TaskSetManager: Lost task 0.3 in stage 4.0 (TID 19) on 192.168.2.113, executor 2: java.io.InvalidClassException (org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982) [duplicate 3]\n",
      "24/03/13 10:06:08 ERROR TaskSetManager: Task 0 in stage 4.0 failed 4 times; aborting job\n",
      "24/03/13 10:06:08 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/13 10:06:08 INFO TaskSchedulerImpl: Cancelling stage 4\n",
      "24/03/13 10:06:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19) (192.168.2.113 executor 2): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n",
      "\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/13 10:06:08 INFO DAGScheduler: ResultStage 4 (runJob at PythonRDD.scala:181) failed in 0.176 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19) (192.168.2.113 executor 2): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n",
      "\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/13 10:06:08 INFO DAGScheduler: Job 4 failed: runJob at PythonRDD.scala:181, took 0.179282 s\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19) (192.168.2.113 executor 2): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lines \u001b[38;5;241m=\u001b[39m spark_context\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ubuntu/i-have-a-dream.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19) (192.168.2.113 executor 2): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/13 10:06:38 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 2\n",
      "24/03/13 10:06:38 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 2\n",
      "24/03/13 10:06:38 INFO ExecutorAllocationManager: Executors 2 removed due to idle timeout.\n",
      "24/03/13 10:06:42 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/03/13 10:06:42 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/13 10:06:42 INFO SparkUI: Stopped Spark web UI at http://waldo-ruiz-master-de-6:4040\n",
      "24/03/13 10:06:42 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/03/13 10:06:42 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/03/13 10:06:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/13 10:06:43 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/13 10:06:43 INFO BlockManager: BlockManager stopped\n",
      "24/03/13 10:06:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/13 10:06:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/13 10:06:43 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/03/13 10:06:43 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/03/13 10:06:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-25c97326-73f7-4f59-ad8f-eb042161f255/pyspark-d70af6aa-a77b-4873-af86-1b63a31891f3\n",
      "24/03/13 10:06:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-f131389b-1f83-461b-90f6-e41874f81d9d\n",
      "24/03/13 10:06:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-25c97326-73f7-4f59-ad8f-eb042161f255\n"
     ]
    }
   ],
   "source": [
    "lines = spark_context.textFile('/home/ubuntu/i-have-a-dream.txt')\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+------------+--------------------+-----------+--------+\n",
      "|            author|                body|             content|content_len|     id|      normalizedBody|           subreddit|subreddit_id|             summary|summary_len|   title|\n",
      "+------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+------------+--------------------+-----------+--------+\n",
      "|  raysofdarkmatter|I think it should...|I think it should...|        178|c69al3r|I think it should...|                math|    t5_2qh0n|Shifting seasonal...|          8|    null|\n",
      "|           Stork13|Art is about the ...|Art is about the ...|        148|c6a9nxd|Art is about the ...|               funny|    t5_2qh33|Personal opinions...|          4|    null|\n",
      "|     Cloud_dreamer|Ask me what I thi...|Ask me what I thi...|         76|c6acx4l|Ask me what I thi...|         Borderlands|    t5_2r8cd|insults and slack...|         73|    null|\n",
      "|     NightlyReaper|In Mechwarrior On...|In Mechwarrior On...|        213|c8onqew|In Mechwarrior On...|            gamingpc|    t5_2sq2y|Yes, Joysticks in...|         19|    null|\n",
      "|    NuffZetPand0ra|You are talking a...|You are talking a...|        404|c6acxvc|You are talking a...|              Diablo|    t5_2qore|Class only items ...|          7|D2 help?|\n",
      "|beatlecreedcabaret|All but one of my...|All but one of my...|        130|c6ahuc4|All but one of my...|   RedditLaqueristas|    t5_2se5q|      OPI Nail Envy!|          3|    null|\n",
      "|      nobodysdiary|I could give a sh...|I could give a sh...|        156|c6aggux|I could give a sh...|               apple|    t5_2qh1f|I don't drive lik...|         18|    null|\n",
      "|          chrom_ed|So you're saying ...|So you're saying ...|        134|c6agxtv|So you're saying ...|               apple|    t5_2qh1f|you don't seem to...|          9|    null|\n",
      "|      gadzookfilms|I love this idea ...|I love this idea ...|        126|c6asb7p|I love this idea ...|RedditFilmsProduc...|    t5_2v33h|How we make money...|          9|    null|\n",
      "|      iamacannibal|Theres an entire ...|Theres an entire ...|        181|c6aveyw|Theres an entire ...|       AbandonedPorn|    t5_2sh6t|I'll try and get ...|         25|    null|\n",
      "+------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+------------+--------------------+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|      normalizedBody|\n",
      "+--------------------+\n",
      "|I think it should...|\n",
      "|Art is about the ...|\n",
      "|Ask me what I thi...|\n",
      "|In Mechwarrior On...|\n",
      "|You are talking a...|\n",
      "|All but one of my...|\n",
      "|I could give a sh...|\n",
      "|So you're saying ...|\n",
      "|I love this idea ...|\n",
      "|Theres an entire ...|\n",
      "|FALSE. Evidence: ...|\n",
      "|If the number of ...|\n",
      "|Yeah, but most fo...|\n",
      "|As an entrepreneu...|\n",
      "|i guess the way I...|\n",
      "|Didn't they lose ...|\n",
      "|You probably won'...|\n",
      "|To simply say tha...|\n",
      "|This picture does...|\n",
      "|And that is, hand...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# root\n",
    "#  |-- author: string (nullable = true) \n",
    "#  |-- body: string (nullable = true)\n",
    "#  |-- content: string (nullable = true)\n",
    "#  |-- content_len: long (nullable = true)\n",
    "#  |-- id: string (nullable = true)\n",
    "#  |-- normalizedBody: string (nullable = true)\n",
    "#  |-- subreddit: string (nullable = true)\n",
    "#  |-- subreddit_id: string (nullable = true)\n",
    "#  |-- summary: string (nullable = true)\n",
    "#  |-- summary_len: long (nullable = true) \n",
    "#  |-- title: string (nullable = true)\n",
    "\n",
    "reddit_data = spark_session.read.json(\"/home/ubuntu/input/sample-2000.json\")\n",
    "reddit_data = reddit_data.drop(\n",
    "    *[\n",
    "        \"content_len\",\n",
    "        \"summary_len\",\n",
    "        \"id\",\n",
    "        \"subreddit_id\",\n",
    "        \"body\",\n",
    "        \"content\",\n",
    "        \"summary\",\n",
    "        \"title\",\n",
    "        \"subreddit\",\n",
    "        \"author\"\n",
    "    ]\n",
    ")\n",
    "reddit_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.cs.cmu.edu/~biglou/resources/bad-words.txt\n",
    "bad_words = spark_context.textFile(\"/home/ubuntu/bad_words.txt\")\n",
    "# bad_words = spark_session.createDataFrame(bad_words, ['bad_words'])\n",
    "bad_words = bad_words.filter(lambda x: x != '').collect()\n",
    "# print(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_data = reddit_data.withColumn(\"splited_words\", F.split(\"normalizedBody\", ' '))\n",
    "reddit_data = reddit_data.filter(F.col('normalizedBody').rlike('|'.join(bad_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|bad_word|count|\n",
      "+--------+-----+\n",
      "|    shit|  176|\n",
      "| fucking|  170|\n",
      "|    fuck|  127|\n",
      "|     god|  103|\n",
      "|    kill|   90|\n",
      "|     kid|   87|\n",
      "|   black|   86|\n",
      "|     sex|   80|\n",
      "|  stupid|   71|\n",
      "|     ass|   66|\n",
      "|    damn|   63|\n",
      "|   fight|   59|\n",
      "|   girls|   56|\n",
      "|     fat|   54|\n",
      "|   death|   52|\n",
      "|     gun|   50|\n",
      "|    hell|   49|\n",
      "|     gay|   49|\n",
      "|  killed|   46|\n",
      "|  fucked|   45|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read bad words from text file\n",
    "bad_words = spark_session.read.text(\"/home/ubuntu/bad_words.txt\") \\\n",
    "    .filter(F.col(\"value\") != \"\") \\\n",
    "    .select(F.col(\"value\").alias(\"bad_word\"))\n",
    "\n",
    "# Extract words from comments and explode them\n",
    "words = reddit_data.select(F.explode(F.split(F.lower(F.col(\"normalizedBody\")), \"\\\\s+\")).alias(\"word\"))\n",
    "\n",
    "# Filter out bad words\n",
    "bad_words_counts = words.join(bad_words, words.word == bad_words.bad_word, \"left_outer\") \\\n",
    "    .filter(F.col(\"bad_word\").isNotNull()) \\\n",
    "    .groupBy(\"bad_word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "# # Show bad words counts\n",
    "bad_words_counts.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark_session.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
