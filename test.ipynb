{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jupyter lab --ip=0.0.0.0\n",
    "# start-master.sh\n",
    "# start-worker.sh spark://yash-kukrejade-6:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.14:7077\") \\\n",
    "        .appName(\"Lecture1_Example0_with_spark\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.host\", \"192.168.2.14\")\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = spark_context.textFile('hdfs://192.168.2.14:9000/home/ubuntu/i-have-a-dream.txt')\n",
    "# lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root\n",
    "#  |-- author: string (nullable = true) \n",
    "#  |-- body: string (nullable = true)\n",
    "#  |-- content: string (nullable = true)\n",
    "#  |-- content_len: long (nullable = true)\n",
    "#  |-- id: string (nullable = true)\n",
    "#  |-- normalizedBody: string (nullable = true)\n",
    "#  |-- subreddit: string (nullable = true)\n",
    "#  |-- subreddit_id: string (nullable = true)\n",
    "#  |-- summary: string (nullable = true)\n",
    "#  |-- summary_len: long (nullable = true) \n",
    "#  |-- title: string (nullable = true)\n",
    "\n",
    "reddit_data = spark_session.read.json(\"hdfs://192.168.2.14:9000/home/ubuntu/input/sample-2000.json\")\n",
    "reddit_data = reddit_data.drop(\n",
    "    *[\n",
    "        \"content_len\",\n",
    "        \"summary_len\",\n",
    "        \"id\",\n",
    "        \"subreddit_id\",\n",
    "        \"body\",\n",
    "        \"content\",\n",
    "        \"summary\",\n",
    "        \"title\",\n",
    "        \"subreddit\",\n",
    "        \"author\"\n",
    "    ]\n",
    ")\n",
    "reddit_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.cs.cmu.edu/~biglou/resources/bad-words.txt\n",
    "bad_words = spark_context.textFile(\"hdfs://192.168.2.14:9000/home/ubuntu/bad_words.txt\")\n",
    "# bad_words = spark_session.createDataFrame(bad_words, ['bad_words'])\n",
    "bad_words = bad_words.filter(lambda x: x != '').collect()\n",
    "# print(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_data = reddit_data.withColumn(\"splited_words\", F.split(\"normalizedBody\", ' '))\n",
    "reddit_data = reddit_data.filter(F.col('normalizedBody').rlike('|'.join(bad_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read bad words from text file\n",
    "bad_words = spark_session.read.text(\"hdfs://192.168.2.14:9000/home/ubuntu/bad_words.txt\") \\\n",
    "    .filter(F.col(\"value\") != \"\") \\\n",
    "    .select(F.col(\"value\").alias(\"bad_word\"))\n",
    "\n",
    "# Extract words from comments and explode them\n",
    "words = reddit_data.select(F.explode(F.split(F.lower(F.col(\"normalizedBody\")), \"\\\\s+\")).alias(\"word\"))\n",
    "\n",
    "# Filter out bad words\n",
    "bad_words_counts = words.join(bad_words, words.word == bad_words.bad_word, \"left_outer\") \\\n",
    "    .filter(F.col(\"bad_word\").isNotNull()) \\\n",
    "    .groupBy(\"bad_word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "# # Show bad words counts\n",
    "bad_words_counts.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark_session.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
